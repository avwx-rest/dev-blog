{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Blog Home The developer blog for the AVWX REST API . About Hi \ud83d\udc4b I'm Michael. I'm the creator and maintainer of AVWX. AVWX started as a Raspberry Pi project in 2014 while finishing my pilot training and has grown organically since converting it into a web API. This blog is meant to improve transparency and act as a development log for my future self. Auth Token Rollout A big change is coming to AVWX. Click the link above. Latest Posts Hurricane Dorian - 2019-09-04 Stripe Checkout Migration - 2019-08-19 Auth Token Rollout - 2019-08-05 Server Migration - 2019-08-04","title":"Blog Home"},{"location":"#blog-home","text":"The developer blog for the AVWX REST API .","title":"Blog Home"},{"location":"#about","text":"Hi \ud83d\udc4b I'm Michael. I'm the creator and maintainer of AVWX. AVWX started as a Raspberry Pi project in 2014 while finishing my pilot training and has grown organically since converting it into a web API. This blog is meant to improve transparency and act as a development log for my future self.","title":"About"},{"location":"#auth-token-rollout","text":"A big change is coming to AVWX. Click the link above.","title":"Auth Token Rollout"},{"location":"#latest-posts","text":"Hurricane Dorian - 2019-09-04 Stripe Checkout Migration - 2019-08-19 Auth Token Rollout - 2019-08-05 Server Migration - 2019-08-04","title":"Latest Posts"},{"location":"2019/08-04/","text":"08-04 Server Migration 2019-08-04 This weekend, I was finally ready to do some much-needed devops work. Problem The main AVWX API servers and cache were hosted in separate regions. This was due to poor planning and just getting things to work in the beginning when I was handling 10k per day instead of per minute. The caching layer was implemented with Azure CosmosDB with a MongoDB client. This worked well for high read, low write caching because CosmosDB pricing is based on writes. However, while you don't need to worry about clusters or migrations, the table throughput would not autoscale. It also would become too expensive to include metric counting because analytics are low read, high write. Solution Replace CosmosDB with traditional DB cluster and host it in the same data center as the API. Process First, I replaced CosmosDB with a MongoDB cluster hosted in Azure US East 2 managed by MongoDB Cloud Atlas. I was already using the Mongo client, so no code changes were necessary. Because CosmosDB was only holding cache data, I did not need to worry about data migration either. Next, I created a new Azure Web App in US East 2 to host the Docker container. The configuration was identical to the previous server, but Azure does not support duplicating a Linux container app between regions. Once the server was properly configured, the A record was updated to point to the new IP address. A happy accident that I found was that traffic slowly began migrating to the new server as the updated record propogated to the various DNS systems. This allowed the new cache to gracefully build up and for me to quickly address any issues that arrose during the switch. Result I now have the API and database in the same region. This cut the average response time from from 48ms down to 11ms . As of this post, 85% of traffic has moved to the new server in the first 16 hours with the old server scaled down to handle the remaining traffic. It will eventually be deleted as the traffic gets near zero. There are still some 5xx errors popping up that inflate the average response time still, but these are erratic and should either disappear or be addressed later as they represent less than 0.001% of traffic. Next Steps Now that the database is more friendly to high write data, I need to enable the station counting system previously tested.","title":"08-04 Server Migration"},{"location":"2019/08-04/#08-04-server-migration","text":"2019-08-04 This weekend, I was finally ready to do some much-needed devops work.","title":"08-04 Server Migration"},{"location":"2019/08-04/#problem","text":"The main AVWX API servers and cache were hosted in separate regions. This was due to poor planning and just getting things to work in the beginning when I was handling 10k per day instead of per minute. The caching layer was implemented with Azure CosmosDB with a MongoDB client. This worked well for high read, low write caching because CosmosDB pricing is based on writes. However, while you don't need to worry about clusters or migrations, the table throughput would not autoscale. It also would become too expensive to include metric counting because analytics are low read, high write.","title":"Problem"},{"location":"2019/08-04/#solution","text":"Replace CosmosDB with traditional DB cluster and host it in the same data center as the API.","title":"Solution"},{"location":"2019/08-04/#process","text":"First, I replaced CosmosDB with a MongoDB cluster hosted in Azure US East 2 managed by MongoDB Cloud Atlas. I was already using the Mongo client, so no code changes were necessary. Because CosmosDB was only holding cache data, I did not need to worry about data migration either. Next, I created a new Azure Web App in US East 2 to host the Docker container. The configuration was identical to the previous server, but Azure does not support duplicating a Linux container app between regions. Once the server was properly configured, the A record was updated to point to the new IP address. A happy accident that I found was that traffic slowly began migrating to the new server as the updated record propogated to the various DNS systems. This allowed the new cache to gracefully build up and for me to quickly address any issues that arrose during the switch.","title":"Process"},{"location":"2019/08-04/#result","text":"I now have the API and database in the same region. This cut the average response time from from 48ms down to 11ms . As of this post, 85% of traffic has moved to the new server in the first 16 hours with the old server scaled down to handle the remaining traffic. It will eventually be deleted as the traffic gets near zero. There are still some 5xx errors popping up that inflate the average response time still, but these are erratic and should either disappear or be addressed later as they represent less than 0.001% of traffic.","title":"Result"},{"location":"2019/08-04/#next-steps","text":"Now that the database is more friendly to high write data, I need to enable the station counting system previously tested.","title":"Next Steps"},{"location":"2019/08-05/","text":"08-05 Auth Token Rollout 2019-08-05 This is a short post detailing the rollout plan to require auth tokens in calls to AVWX. tl;dr Make a free account on the account page Generate your (free) token Start including the Authorization header or token URL parameter November 1st, 2019 : Tokens will be required December 1st, 2019 : Daily rate limits are assigned January 1st, 2020 : Daily rate limits will be enforced Problem Currently, I have no insight into how users are using the API beyond aggregate statistics and email correspondence. I would be able to There is also the economic issue that the API's growth has been unsustainable based on donations and feature-based offerings. About 95% of all traffic is for METARs which is a free tier feature. Introducing rate limits to tiers should help make AVWX break-even while remaining free for most users. Solution API tokens will be required to call AVWX starting in November. This is to allow time to gather usage data to determine the daily rate limits for each tier. Here are the important dates: November 1st, 2019 : Tokens will be required December 1st, 2019 : Rate limits are assigned January 1st, 2020 : Rate limits will be enforced This should hopefully be enough time to update your apps, choose the appropriate usage tier, and register any concerns with me about this whole process. Part of the last account update included the ability to create custom tiers, so I can create custom tiers and prices for specific use cases. I am going to include an open-source tier. My plan as of this post is for it to contain only free tier features but have to rate limit of the lowest paid tier. I also plan to have rate limits for every tier. If a company is using more than the enterprise plan allows, we'll work together on a plan that fits their exact use case. I chose daily rate limits instead of hourly or smaller because aviation reports are requested over the course of an entire day and mostly called dawn to dusk. Hourly rate limits would restrict usage during the day and go unused at night. Steps So Far I made an update to the account portal allowing users to select the free tier as an active plan and upgrade/downgrade to it as well without losing customer info. Free tiers can generate a token. On the API side, token validation now checks the plan associated with the token to determine fulfillment eligibility instead of merely token existence. Token-based call counting is already implemented. Updated 2019-09-09 All API endpoints now check for token availability for logging purposes. Checks are currently disabled for missing tokens. API endpoints now can use allowed plan types as access controls. Tokens can also be supplied as a token URL parameter but will still use the header value first if found. Updated 2019-10-01 Auth tokens are now required to make any call from the API. Todo Roughly in order: [X] Divest the token auth check from specific intents via endpoint.example and instead use something like endpoint.allowed_plans to assign plans to API features. [X] Generate example responses for METAR, TAF, and station endpoints [X] Enforce the Authorization header for all calls [ ] Determine rate limits from token call usage [ ] Announce rate limits [ ] Add rate limits to plan data [ ] Enforce daily rate limits against token call counts","title":"08-05 Auth Token Rollout"},{"location":"2019/08-05/#08-05-auth-token-rollout","text":"2019-08-05 This is a short post detailing the rollout plan to require auth tokens in calls to AVWX.","title":"08-05 Auth Token Rollout"},{"location":"2019/08-05/#tldr","text":"Make a free account on the account page Generate your (free) token Start including the Authorization header or token URL parameter November 1st, 2019 : Tokens will be required December 1st, 2019 : Daily rate limits are assigned January 1st, 2020 : Daily rate limits will be enforced","title":"tl;dr"},{"location":"2019/08-05/#problem","text":"Currently, I have no insight into how users are using the API beyond aggregate statistics and email correspondence. I would be able to There is also the economic issue that the API's growth has been unsustainable based on donations and feature-based offerings. About 95% of all traffic is for METARs which is a free tier feature. Introducing rate limits to tiers should help make AVWX break-even while remaining free for most users.","title":"Problem"},{"location":"2019/08-05/#solution","text":"API tokens will be required to call AVWX starting in November. This is to allow time to gather usage data to determine the daily rate limits for each tier. Here are the important dates: November 1st, 2019 : Tokens will be required December 1st, 2019 : Rate limits are assigned January 1st, 2020 : Rate limits will be enforced This should hopefully be enough time to update your apps, choose the appropriate usage tier, and register any concerns with me about this whole process. Part of the last account update included the ability to create custom tiers, so I can create custom tiers and prices for specific use cases. I am going to include an open-source tier. My plan as of this post is for it to contain only free tier features but have to rate limit of the lowest paid tier. I also plan to have rate limits for every tier. If a company is using more than the enterprise plan allows, we'll work together on a plan that fits their exact use case. I chose daily rate limits instead of hourly or smaller because aviation reports are requested over the course of an entire day and mostly called dawn to dusk. Hourly rate limits would restrict usage during the day and go unused at night.","title":"Solution"},{"location":"2019/08-05/#steps-so-far","text":"I made an update to the account portal allowing users to select the free tier as an active plan and upgrade/downgrade to it as well without losing customer info. Free tiers can generate a token. On the API side, token validation now checks the plan associated with the token to determine fulfillment eligibility instead of merely token existence. Token-based call counting is already implemented. Updated 2019-09-09 All API endpoints now check for token availability for logging purposes. Checks are currently disabled for missing tokens. API endpoints now can use allowed plan types as access controls. Tokens can also be supplied as a token URL parameter but will still use the header value first if found. Updated 2019-10-01 Auth tokens are now required to make any call from the API.","title":"Steps So Far"},{"location":"2019/08-05/#todo","text":"Roughly in order: [X] Divest the token auth check from specific intents via endpoint.example and instead use something like endpoint.allowed_plans to assign plans to API features. [X] Generate example responses for METAR, TAF, and station endpoints [X] Enforce the Authorization header for all calls [ ] Determine rate limits from token call usage [ ] Announce rate limits [ ] Add rate limits to plan data [ ] Enforce daily rate limits against token call counts","title":"Todo"},{"location":"2019/08-19/","text":"08-19 Stripe Checkout Migration 2019-08-19 Spent the weekend migrating the payment processing system to use Stripe's new Checkout portal. Problem Stripe's modal Checkout window which is being depricated. AVWX uses this for credit card entry and storage when upgrading to a paid account. Solution Migrate to the full-page Checkout which will continue being supported and provides additional payment options. Process Stripe offers a Checkout migration guide to start with. I determined that I was using the dynamic subscription method which requires both frontend and backend changes. This would replace the plans.new_subscription and \"Subscribe\" button calls with some additional handling. After making the new \"Subcribe\" button, the page sends a Checkout Session object and redirects the user to the payment page. On success or cancel, Stripe redirects the user to a success or failure endpoint back in the app. They warn not to use the success page to fulfill the request since it doesn't guarentee that the payment went through. They offer a few ways to verify a completed Checkout Session. I opted to register a webhook that would be triggered after a completed session. It first verifies the session in the POST with Stripe to stop any fraudulent calls. It then applies the new customer ID, subscription ID, and plan ID to the user. The user is tracked through this process by passing the user ID to the Check Session object. I tried to use email and existing customer IDs, but these did not guarentee the same user would be found during fulfillment. Stripe sometimes changed the customer ID, and supplying just the email for existing users would create duplicates. I still pass Stripe the email for new customers so I can associate accounts in the Stripe dashboard during support requests. During this process, I also found a bug that prevented free tier users from generating their first token. This was a remnant check when users needed paid accounts to generate a token. This check logic has been fixed and moved into the User class instead of the token view. Result The payment system now redirects users to Stripe to enter their credit card information. Users can now also use Apple Pay with more options available as Stripe adds them with no code changes needed on my part. This upgrade does not affect paid-to-paid and paid-to-free changes. I also fixed a nasty bug for new users. Next Steps A user mentioned that there is no method to update their credit card information without affecting their current plan. That is the next feature I need to address.","title":"08-19 Stripe Checkout Migration"},{"location":"2019/08-19/#08-19-stripe-checkout-migration","text":"2019-08-19 Spent the weekend migrating the payment processing system to use Stripe's new Checkout portal.","title":"08-19 Stripe Checkout Migration"},{"location":"2019/08-19/#problem","text":"Stripe's modal Checkout window which is being depricated. AVWX uses this for credit card entry and storage when upgrading to a paid account.","title":"Problem"},{"location":"2019/08-19/#solution","text":"Migrate to the full-page Checkout which will continue being supported and provides additional payment options.","title":"Solution"},{"location":"2019/08-19/#process","text":"Stripe offers a Checkout migration guide to start with. I determined that I was using the dynamic subscription method which requires both frontend and backend changes. This would replace the plans.new_subscription and \"Subscribe\" button calls with some additional handling. After making the new \"Subcribe\" button, the page sends a Checkout Session object and redirects the user to the payment page. On success or cancel, Stripe redirects the user to a success or failure endpoint back in the app. They warn not to use the success page to fulfill the request since it doesn't guarentee that the payment went through. They offer a few ways to verify a completed Checkout Session. I opted to register a webhook that would be triggered after a completed session. It first verifies the session in the POST with Stripe to stop any fraudulent calls. It then applies the new customer ID, subscription ID, and plan ID to the user. The user is tracked through this process by passing the user ID to the Check Session object. I tried to use email and existing customer IDs, but these did not guarentee the same user would be found during fulfillment. Stripe sometimes changed the customer ID, and supplying just the email for existing users would create duplicates. I still pass Stripe the email for new customers so I can associate accounts in the Stripe dashboard during support requests. During this process, I also found a bug that prevented free tier users from generating their first token. This was a remnant check when users needed paid accounts to generate a token. This check logic has been fixed and moved into the User class instead of the token view.","title":"Process"},{"location":"2019/08-19/#result","text":"The payment system now redirects users to Stripe to enter their credit card information. Users can now also use Apple Pay with more options available as Stripe adds them with no code changes needed on my part. This upgrade does not affect paid-to-paid and paid-to-free changes. I also fixed a nasty bug for new users.","title":"Result"},{"location":"2019/08-19/#next-steps","text":"A user mentioned that there is no method to update their credit card information without affecting their current plan. That is the next feature I need to address.","title":"Next Steps"},{"location":"2019/09-04/","text":"09-04 Hurricane Dorian 2019-09-04 Hurricane Dorian just rolled by Central Florida where I live. Everything has been fine here with only 60 mph winds reported at Kennedy Space Center. However, the Northern Bahamas have been devestated by the storm after experiencing two full days of category 5 winds (>155 mph) with its peak at 185 mph. Having experienced cat5 winds in the past, my heart goes out to all those who are now living with Dorian's aftermath. Freeport's Grand Bahama International Airport (MYGF) has been completely destroyed after storm surge submerged the tarmac in at least 10 feet of sea water. This video shows the aftermath: I'd been watching the reports coming out of MYGF leading up to the storm. The final METAR was uneventful showing VCSH and low ENE winds as the only indicators the storm was coming. While the airport is still sending TAFs, this is likely the last METAR we'll see from MYGF for some time: { \"meta\": { \"timestamp\": \"2019-09-05T00:26:41.452307Z\", \"cache-timestamp\": \"2019-09-01T02:05:35.740000Z\", \"warning\": \"Unable to fetch report. This cached data might be out of date. To return an error instead, set ?onfail=error\" }, \"altimeter\": { \"repr\": \"2991\", \"value\": 29.91, \"spoken\": \"two nine point nine one\" }, \"clouds\": [ { \"repr\": \"FEW012CB\", \"type\": \"FEW\", \"altitude\": 12, \"modifier\": \"CB\", \"direction\": null }, { \"repr\": \"BKN220\", \"type\": \"BKN\", \"altitude\": 220, \"modifier\": null, \"direction\": null } ], \"flight_rules\": \"VFR\", \"other\": [ \"VCSH\" ], \"sanitized\": \"MYGF 010000Z 06008KT 9999 VCSH FEW012CB BKN220 28/24 A2991 \", \"visibility\": { \"repr\": \"9999\", \"value\": 9999, \"spoken\": \"nine nine nine nine\" }, \"wind_direction\": { \"repr\": \"060\", \"value\": 60, \"spoken\": \"zero six zero\" }, \"wind_gust\": null, \"wind_speed\": { \"repr\": \"08\", \"value\": 8, \"spoken\": \"eight\" }, \"raw\": \"MYGF 010000Z 06008KT 9999 VCSH FEW012CB BKN220 28/24 A2991\", \"station\": \"MYGF\", \"time\": { \"repr\": \"010000Z\", \"dt\": \"2019-09-01T00:00:00Z\" }, \"remarks\": \"\", \"dewpoint\": { \"repr\": \"24\", \"value\": 24, \"spoken\": \"two four\" }, \"remarks_info\": { \"dewpoint_decimal\": null, \"temperature_decimal\": null }, \"runway_visibility\": [], \"temperature\": { \"repr\": \"28\", \"value\": 28, \"spoken\": \"two eight\" }, \"wind_variable_direction\": [], \"units\": { \"altimeter\": \"inHg\", \"altitude\": \"ft\", \"temperature\": \"C\", \"visibility\": \"m\", \"wind_speed\": \"kt\" } }","title":"09-04 Hurricane Dorian"},{"location":"2019/09-04/#09-04-hurricane-dorian","text":"2019-09-04 Hurricane Dorian just rolled by Central Florida where I live. Everything has been fine here with only 60 mph winds reported at Kennedy Space Center. However, the Northern Bahamas have been devestated by the storm after experiencing two full days of category 5 winds (>155 mph) with its peak at 185 mph. Having experienced cat5 winds in the past, my heart goes out to all those who are now living with Dorian's aftermath. Freeport's Grand Bahama International Airport (MYGF) has been completely destroyed after storm surge submerged the tarmac in at least 10 feet of sea water. This video shows the aftermath: I'd been watching the reports coming out of MYGF leading up to the storm. The final METAR was uneventful showing VCSH and low ENE winds as the only indicators the storm was coming. While the airport is still sending TAFs, this is likely the last METAR we'll see from MYGF for some time: { \"meta\": { \"timestamp\": \"2019-09-05T00:26:41.452307Z\", \"cache-timestamp\": \"2019-09-01T02:05:35.740000Z\", \"warning\": \"Unable to fetch report. This cached data might be out of date. To return an error instead, set ?onfail=error\" }, \"altimeter\": { \"repr\": \"2991\", \"value\": 29.91, \"spoken\": \"two nine point nine one\" }, \"clouds\": [ { \"repr\": \"FEW012CB\", \"type\": \"FEW\", \"altitude\": 12, \"modifier\": \"CB\", \"direction\": null }, { \"repr\": \"BKN220\", \"type\": \"BKN\", \"altitude\": 220, \"modifier\": null, \"direction\": null } ], \"flight_rules\": \"VFR\", \"other\": [ \"VCSH\" ], \"sanitized\": \"MYGF 010000Z 06008KT 9999 VCSH FEW012CB BKN220 28/24 A2991 \", \"visibility\": { \"repr\": \"9999\", \"value\": 9999, \"spoken\": \"nine nine nine nine\" }, \"wind_direction\": { \"repr\": \"060\", \"value\": 60, \"spoken\": \"zero six zero\" }, \"wind_gust\": null, \"wind_speed\": { \"repr\": \"08\", \"value\": 8, \"spoken\": \"eight\" }, \"raw\": \"MYGF 010000Z 06008KT 9999 VCSH FEW012CB BKN220 28/24 A2991\", \"station\": \"MYGF\", \"time\": { \"repr\": \"010000Z\", \"dt\": \"2019-09-01T00:00:00Z\" }, \"remarks\": \"\", \"dewpoint\": { \"repr\": \"24\", \"value\": 24, \"spoken\": \"two four\" }, \"remarks_info\": { \"dewpoint_decimal\": null, \"temperature_decimal\": null }, \"runway_visibility\": [], \"temperature\": { \"repr\": \"28\", \"value\": 28, \"spoken\": \"two eight\" }, \"wind_variable_direction\": [], \"units\": { \"altimeter\": \"inHg\", \"altitude\": \"ft\", \"temperature\": \"C\", \"visibility\": \"m\", \"wind_speed\": \"kt\" } }","title":"09-04 Hurricane Dorian"}]}