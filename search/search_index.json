{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Blog Home The developer blog for the AVWX REST API . About Hi \ud83d\udc4b I'm Michael. I'm the creator and maintainer of AVWX. AVWX started as a Raspberry Pi project in 2014 while finishing my pilot training and has grown organically since converting it into a web API. This blog is meant to improve transparency and act as a development log for my future self. Auth Token Rollout A big change is coming to AVWX. Click the link above. Latest Posts Auth Token Rollout - 2019-08-05 Server Migration - 2019-08-04","title":"Blog Home"},{"location":"#blog-home","text":"The developer blog for the AVWX REST API .","title":"Blog Home"},{"location":"#about","text":"Hi \ud83d\udc4b I'm Michael. I'm the creator and maintainer of AVWX. AVWX started as a Raspberry Pi project in 2014 while finishing my pilot training and has grown organically since converting it into a web API. This blog is meant to improve transparency and act as a development log for my future self.","title":"About"},{"location":"#auth-token-rollout","text":"A big change is coming to AVWX. Click the link above.","title":"Auth Token Rollout"},{"location":"#latest-posts","text":"Auth Token Rollout - 2019-08-05 Server Migration - 2019-08-04","title":"Latest Posts"},{"location":"2019/08-04/","text":"08-04 Server Migration 2019-08-04 This weekend, I was finally ready to do some much-needed devops work. Problem The main AVWX API servers and cache were hosted in separate regions. This was due to poor planning and just getting things to work in the beginning when I was handling 10k per day instead of per minute. The caching layer was implemented with Azure CosmosDB with a MongoDB client. This worked well for high read, low write caching because CosmosDB pricing is based on writes. However, while you don't need to worry about clusters or migrations, the table throughput would not autoscale. It also would become too expensive to include metric counting because analytics are low read, high write. Solution Replace CosmosDB with traditional DB cluster and host it in the same data center as the API. Process First, I replaced CosmosDB with a MongoDB cluster hosted in Azure US East 2 managed by MongoDB Cloud Atlas. I was already using the Mongo client, so no code changes were necessary. Because CosmosDB was only holding cache data, I did not need to worry about data migration either. Next, I created a new Azure Web App in US East 2 to host the Docker container. The configuration was identical to the previous server, but Azure does not support duplicating a Linux container app between regions. Once the server was properly configured, the A record was updated to point to the new IP address. A happy accident that I found was that traffic slowly began migrating to the new server as the updated record propogated to the various DNS systems. This allowed the new cache to gracefully build up and for me to quickly address any issues that arrose during the switch. Result I now have the API and database in the same region. This cut the average response time from from 48ms down to 11ms . As of this post, 85% of traffic has moved to the new server in the first 16 hours with the old server scaled down to handle the remaining traffic. It will eventually be deleted as the traffic gets near zero. There are still some 5xx errors popping up that inflate the average response time still, but these are erratic and should either disappear or be addressed later as they represent less than 0.001% of traffic. Next Steps Now that the database is more friendly to high write data, I need to enable the station counting system previously tested.","title":"08-04 Server Migration"},{"location":"2019/08-04/#08-04-server-migration","text":"2019-08-04 This weekend, I was finally ready to do some much-needed devops work.","title":"08-04 Server Migration"},{"location":"2019/08-04/#problem","text":"The main AVWX API servers and cache were hosted in separate regions. This was due to poor planning and just getting things to work in the beginning when I was handling 10k per day instead of per minute. The caching layer was implemented with Azure CosmosDB with a MongoDB client. This worked well for high read, low write caching because CosmosDB pricing is based on writes. However, while you don't need to worry about clusters or migrations, the table throughput would not autoscale. It also would become too expensive to include metric counting because analytics are low read, high write.","title":"Problem"},{"location":"2019/08-04/#solution","text":"Replace CosmosDB with traditional DB cluster and host it in the same data center as the API.","title":"Solution"},{"location":"2019/08-04/#process","text":"First, I replaced CosmosDB with a MongoDB cluster hosted in Azure US East 2 managed by MongoDB Cloud Atlas. I was already using the Mongo client, so no code changes were necessary. Because CosmosDB was only holding cache data, I did not need to worry about data migration either. Next, I created a new Azure Web App in US East 2 to host the Docker container. The configuration was identical to the previous server, but Azure does not support duplicating a Linux container app between regions. Once the server was properly configured, the A record was updated to point to the new IP address. A happy accident that I found was that traffic slowly began migrating to the new server as the updated record propogated to the various DNS systems. This allowed the new cache to gracefully build up and for me to quickly address any issues that arrose during the switch.","title":"Process"},{"location":"2019/08-04/#result","text":"I now have the API and database in the same region. This cut the average response time from from 48ms down to 11ms . As of this post, 85% of traffic has moved to the new server in the first 16 hours with the old server scaled down to handle the remaining traffic. It will eventually be deleted as the traffic gets near zero. There are still some 5xx errors popping up that inflate the average response time still, but these are erratic and should either disappear or be addressed later as they represent less than 0.001% of traffic.","title":"Result"},{"location":"2019/08-04/#next-steps","text":"Now that the database is more friendly to high write data, I need to enable the station counting system previously tested.","title":"Next Steps"},{"location":"2019/08-05/","text":"08-05 Auth Token Rollout 2019-08-05 This is a short post detailing the rollout plan to require auth tokens in calls to AVWX. tl;dr Make a free account on the account page Generate your (free) token Start including the header Authorization: BEARER MY_TOKEN_VALUE November 1st, 2019 : Tokens will be required December 1st, 2019 : Daily rate limits are assigned January 1st, 2020 : Daily rate limits will be enforced Problem Currently, I have no insight into how users are using the API beyond aggregate statistics and email correspondence. I would be able to There is also the economic issue that the API's growth has been unsustainable based on donations and feature-based offerings. About 95% of all traffic is for METARs which is a free tier feature. Introducing rate limits to tiers should help make AVWX break-even while remaining free for most users. Solution API tokens will be required to call AVWX starting in November. This is to allow time to gather usage data to determine the daily rate limits for each tier. Here are the important dates: November 1st, 2019 : Tokens will be required December 1st, 2019 : Rate limits are assigned January 1st, 2020 : Rate limits will be enforced This should hopefully be enough time to update your apps, choose the appropriate usage tier, and register any concerns with me about this whole process. Part of the last account update included the ability to create custom tiers, so I can create custom tiers and prices for specific use cases. I am going to include an open-source tier. My plan as of this post is for it to contain only free tier features but have to rate limit of the lowest paid tier. I also plan to have rate limits for every tier. If a company is using more than the enterprise plan allows, we'll work together on a plan that fits their exact use case. I chose daily rate limits instead of hourly or smaller because aviation reports are requested over the course of an entire day and mostly called dawn to dusk. Hourly rate limits would restrict usage during the day and go unused at night. Steps So Far I made an update to the account portal allowing users to select the free tier as an active plan and upgrade/downgrade to it as well without losing customer info. Free tiers can generate a token. On the API side, token validation now checks the plan associated with the token to determine fulfillment eligibility instead of merely token existance. Token-based call counting is already implemeted. Todo Roughly in order: Divest the token auth check from specific intents via endpoint.example and instead use something like endpoint.allowed_plans to assign plans to API features. Generate example responses for METAR, TAF, and station endpoints Enforce the Authorization header for all calls Determine rate limits from token call usage Announce rate limits Add rate limits to plan data Enforce daily rate limits against token call counts","title":"08-05 Auth Token Rollout"},{"location":"2019/08-05/#08-05-auth-token-rollout","text":"2019-08-05 This is a short post detailing the rollout plan to require auth tokens in calls to AVWX.","title":"08-05 Auth Token Rollout"},{"location":"2019/08-05/#tldr","text":"Make a free account on the account page Generate your (free) token Start including the header Authorization: BEARER MY_TOKEN_VALUE November 1st, 2019 : Tokens will be required December 1st, 2019 : Daily rate limits are assigned January 1st, 2020 : Daily rate limits will be enforced","title":"tl;dr"},{"location":"2019/08-05/#problem","text":"Currently, I have no insight into how users are using the API beyond aggregate statistics and email correspondence. I would be able to There is also the economic issue that the API's growth has been unsustainable based on donations and feature-based offerings. About 95% of all traffic is for METARs which is a free tier feature. Introducing rate limits to tiers should help make AVWX break-even while remaining free for most users.","title":"Problem"},{"location":"2019/08-05/#solution","text":"API tokens will be required to call AVWX starting in November. This is to allow time to gather usage data to determine the daily rate limits for each tier. Here are the important dates: November 1st, 2019 : Tokens will be required December 1st, 2019 : Rate limits are assigned January 1st, 2020 : Rate limits will be enforced This should hopefully be enough time to update your apps, choose the appropriate usage tier, and register any concerns with me about this whole process. Part of the last account update included the ability to create custom tiers, so I can create custom tiers and prices for specific use cases. I am going to include an open-source tier. My plan as of this post is for it to contain only free tier features but have to rate limit of the lowest paid tier. I also plan to have rate limits for every tier. If a company is using more than the enterprise plan allows, we'll work together on a plan that fits their exact use case. I chose daily rate limits instead of hourly or smaller because aviation reports are requested over the course of an entire day and mostly called dawn to dusk. Hourly rate limits would restrict usage during the day and go unused at night.","title":"Solution"},{"location":"2019/08-05/#steps-so-far","text":"I made an update to the account portal allowing users to select the free tier as an active plan and upgrade/downgrade to it as well without losing customer info. Free tiers can generate a token. On the API side, token validation now checks the plan associated with the token to determine fulfillment eligibility instead of merely token existance. Token-based call counting is already implemeted.","title":"Steps So Far"},{"location":"2019/08-05/#todo","text":"Roughly in order: Divest the token auth check from specific intents via endpoint.example and instead use something like endpoint.allowed_plans to assign plans to API features. Generate example responses for METAR, TAF, and station endpoints Enforce the Authorization header for all calls Determine rate limits from token call usage Announce rate limits Add rate limits to plan data Enforce daily rate limits against token call counts","title":"Todo"}]}